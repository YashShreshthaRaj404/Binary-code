![logo](https://github.com/YashShreshthaRaj404/binary_code/blob/main/binary-ndeaveyqv9hps1g9.jpg)

#### Binary Code
is the most basic form of computer code, consisting of two numbers: 0 and 1. These numbers form the basic layer of all computing systems and are the primary language of digital technologies. Binary code uses combinations of these two numbers to represent numbers, letters, or other types of information.



## Decimal numerals represented by binary Digits

#### decimal	binary	conversion...

0	0	0 ( 20 )
1	1	1 ( 20 )
2	10	1 ( 21 ) + 0 ( 20 )
3	11	1 ( 21 ) + 1 ( 20 )
4	100	1 ( 22 ) + 0 ( 21 ) + 0 ( 20 )
5	101	1 ( 22 ) + 0 ( 21 ) + 1 ( 20 )
6	110	1 ( 22 ) + 1 ( 21 ) + 0 ( 20 )
7	111	1 ( 22 ) + 1 ( 21 ) + 1 ( 20 )
8	1000	1 ( 23 ) + 0 ( 22 ) + 0 ( 21 ) + 0 ( 20 )
9	1001	1 ( 23 ) + 0 ( 22 ) + 0 ( 21 ) + 1 ( 20 )
10	1010	1 ( 23 ) + 0 ( 22 ) + 1 ( 21 ) + 0 ( 20 )

## Importance of binary code
The binary number system is the base of all computing systems and operations. It enables devices to store, access and manipulate all types of information directed to and from the CPU or memory. This makes it possible to develop applications that enable users to do the following:

•view websites;
•create and update documents;
•play games
•view streaming video and other kinds of graphical information
•access software and 
perform calculations and data analyses.

##### The binary schema of digital 1s and 0s offers a simple and elegant way for computers to work. It also offers an efficient way to control logic circuits and to detect an electrical signal's true (1) and false (0) states.